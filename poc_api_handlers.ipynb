{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the answers:\n",
      "\n",
      "**Hindi:** फ्रांस की राजधानी पेरिस है। (Frāns kī rājdhānī pērīs h.)\n",
      "\n",
      "**French:** La capitale de la France est Paris.\n",
      "\n",
      "Let me know if you need anything else!\n"
     ]
    }
   ],
   "source": [
    "##Groq POC\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "model = \"llama3-70b-8192\"\n",
    "system_prompt = {\"role\": \"system\", \"content\": \"you are a helpful assistant\"}\n",
    "generation_configs = {\n",
    "    \"temperature\": 1,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"top_p\": 1,\n",
    "    \"stream\": False,\n",
    "    \"stop\": None,\n",
    "}\n",
    "user_prompt = {\"role\": \"user\", \"content\": \"What is the capital of France? Respond in Hindi and French.\"}\n",
    "completion = client.chat.completions.create(messages=[system_prompt, user_prompt], model=model, **generation_configs)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi: फ्रांस की राजधानी पेरिस है।\n",
      "French: La capitale de la France est Paris.\n"
     ]
    }
   ],
   "source": [
    "##OpenAI POC\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "gpt_model = \"gpt-4o-2024-05-13\"\n",
    "gpt_response = openai_client.chat.completions.create(\n",
    "    model=gpt_model,\n",
    "    messages=[system_prompt, user_prompt],\n",
    "    **generation_configs\n",
    ")\n",
    "\n",
    "print(gpt_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He gato, es tiempo de meow meow."
     ]
    }
   ],
   "source": [
    "##Replicate POC with FLAN T5 XL\n",
    "import replicate\n",
    "import asyncio\n",
    "model_flant5 = \"replicate/flan-t5-xl:7a216605843d87f5426a10d2cc6940485a232336ed04d655ef86b91e020e9210\"\n",
    "user_prompt = \"Translate the following sentence to spanish: Hey cat, It's time to meow meow.\"\n",
    "input = {\n",
    "    \"top_p\": 1,\n",
    "    \"max_length\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"prompt\": user_prompt,\n",
    "    \"debug\": False,\n",
    "}\n",
    "\n",
    "\n",
    "output = replicate.run(model_flant5, input)\n",
    "\n",
    "\n",
    "for item in output:\n",
    "    print(item, end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El gato está en la mesa."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
